{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lab Instruction\n",
    "\n",
    "In this notebook, we will learn how to implement DQN using Tensorflow for the [Cartpole environment in OpenAI gym](https://gym.openai.com/envs/CartPole-v0/). The notebook contains all the info that you need to understand the basic mechanism of DQN, you could also refer to the DQN pdf for detailed pseudocode. **Your task** is to fill in the\n",
    "\n",
    "YOUR CODE HERE\n",
    "\n",
    "sections in the code blocks below, to complete the building of computation graph, and the implementation of other parts of DQN. You are free to tweak all codes except the last block. Your are also free to tweak the hyper-parameters to improve the performance of the agent. The final block of the code evaluates the performance of the agent on an independent 100 episodes on the environment and print out the average testing performance.\n",
    "\n",
    "Make sure that your final submission is a notebook that can be run from beginning to end, and you should print out the accuracy at the end of the notebook (i.e. be sure to run the last block after training). Your agent's performance is expected to reach near 200 at the end of training. **Your grade will depend on the final evaluation performance of the agent**. However, if you tweak the code to report false result, you will receive no credit for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN (Deep Q Network)\n",
    "\n",
    "In previous Labs, we have learned to use Tensorflow to build deep learning models. In this lab, we will apply deep learning as function approximations in reinforcement learning. \n",
    "\n",
    "Reference: DQN https://arxiv.org/abs/1312.5602\n",
    "\n",
    "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$ \n",
    "\n",
    "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
    "\n",
    "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*or some features of state and action*). Let $Q_\\theta(s,a)$ be the output of the network, which estimates the optimal Q-value function for state $s$ and action $a$.\n",
    "[//]: $$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman optimality equation\n",
    "\n",
    "We wish to train the neural network in order to find $\\theta$ such that $Q_\\theta(s,a)$ approximates $Q^*(s,a)$. \n",
    "We will use Bellman optimality equation to do this. Recall that for optimal Q function $Q^\\ast(s,a)$ the following holds for all $(s,a)$\n",
    "\n",
    "$$Q^\\ast(s,a) = \\max_{a'} \\mathbb{E}_{s'}\\big[R(s,a) + \\gamma Q^\\ast(s',a)\\big]$$\n",
    "\n",
    "We can use observations of form $(s_i, a_i, r_i, s'_{i})$ (i.e., observing reward $r_i$ and new state $s'_{i}$ on taking action $a_i$ in state $s_i$) for training. Let  $target_i = \\max_a \\mathbb{E}\\big[r_i + \\gamma Q^\\ast(s'_{i},a)\\big]$\n",
    "\n",
    "Then, Q-learning with function approximation uses online gradient descent to find $\\theta$ that minimizes the loss function\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - target_i)^2$$\n",
    "\n",
    "Let us first build a neural network $Q_\\theta(s,a)$ as required above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define neural net Q_\\theta(s,a) as a class\n",
    "\n",
    "class Qfunction(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, sess, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        sess: sess to execute this Qfunction\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # build the prediction graph\n",
    "        state = tf.placeholder(tf.float32, [None, obssize])\n",
    "        Qvalues = None  # make sure it has size [None, actsize]\n",
    "        \n",
    "        # build the targets and actions\n",
    "        # targets represent the terms E[r+gamma Q] in Bellman equations\n",
    "        # actions represent a_t\n",
    "        targets = tf.placeholder(tf.float32, [None])\n",
    "        actions = tf.placeholder(tf.int32, [None])\n",
    "        Qpreds = None  # make sure it has size [None]\n",
    "        loss = tf.reduce_mean(tf.square(Qpreds - targets))\n",
    "\n",
    "        # optimization\n",
    "        self.train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        # some bookkeeping\n",
    "        self.Qvalues = Qvalues\n",
    "        self.state = state\n",
    "        self.actions = actions\n",
    "        self.targets = targets\n",
    "        self.loss = loss\n",
    "        self.sess = sess\n",
    "    \n",
    "    def compute_Qvalues(self, states):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to the neural net, states should have\n",
    "        size [numsamples, obssize], where numsamples is the number of samples\n",
    "        output: Q values for these states. The output should have size \n",
    "        [numsamples, actsize] as numpy array\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.Qvalues, feed_dict={self.state: states})\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        return self.sess.run([self.loss,self.train_op], feed_dict={self.state:states, self.actions:actions, self.targets:targets})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have $Q_\\theta(s,a)$ we can execute policies in the environment as follows ($\\epsilon-$greedy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just pseudocode\n",
    "# the following lines illustrate how DQN agent takes actions \n",
    "# to interact with the environment\n",
    "\n",
    "#raise ValueError(\"cannot attemp to run pseudocode\")\n",
    "\n",
    "#env = gym.make('CartPole-v0')\n",
    "#epsilon = .1\n",
    "\n",
    "#obs = env.reset()\n",
    "#done = False\n",
    "#rewardsum = 0\n",
    "\n",
    "#while not done:\n",
    "#    if np.random.rand() < epsilon:\n",
    "#        action = env.action_space.sample()\n",
    "#    else:\n",
    "#        Q = Qfunction.compute_Qvalues(obs)\n",
    "#        action = np.argmax(Q)  # need some tweeking of code here\n",
    "#    \n",
    "#    obs, reward, done, info = env.step(action)\n",
    "#    rewardsum += reward\n",
    "#    \n",
    "#print(\"reward under current policy %f \".format(rewardsum)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can hence collect a bunch of samples $(s_t,a_t,r_t,s_{t+1})$, and compute the targets using the current network. Let the target be $d_i$ as the $i$th target\n",
    "\n",
    "$$d_i = \\max_a r_t + \\gamma Q_\\theta(s_{t+1},a)$$\n",
    "\n",
    "And then feed this value into the computational graph and minimize the Bellman error loss. This procedure has been shown to be fairly unstable. The reference paper has offered two techniques to stabilize the training process: target network and replay buffer.\n",
    "\n",
    "**Replay Buffer**\n",
    "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$, when we minimize the Bellman error. When optimizing the Bellman error loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples from buffer $(s_t,a_t,r_t,s_{t+1}) \\sim R$ and then compute\n",
    "loss \n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - \\max_a (r_i + \\gamma Q_\\theta(s_i^\\prime,a))^2$$\n",
    "\n",
    "and update parameters using backprop.\n",
    "\n",
    "**Target Network**\n",
    "Maintain a target network in addition to the original principal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta_{\\text{target}}$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
    "\n",
    "$$d_i = \\max_a r_t + \\gamma Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
    "\n",
    "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence several critical parts of the complete pseudocode for DQN is as follows:\n",
    "\n",
    "**Initialization**: principal network $Q_\\theta(s,a)$, target network $Q_{\\theta^{-}}(s,a)$. Replay buffer $R = \\{\\}$ (empty). \n",
    "\n",
    "**At each time step $t$**: execute action using $\\epsilon-$greedy based on the principal network $Q_\\theta(s,a)$. To update $\\theta$: sample $N$ tuples $(s_i,a_i,r_i,s_i^\\prime) \\sim R$, compute empirical loss \n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - \\max_a (r_i + \\gamma Q_{\\theta^{-}}(s_i^\\prime,a))^2$$\n",
    "\n",
    "and update parameter $\\theta$ using backprop (just take one gradient step).\n",
    "\n",
    "**Update target network**: Every $\\tau$ time steps, update target network by copying $\\theta_{\\text{target}} \\leftarrow \\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return None  # need implementation\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a subroutine to update target network i.e. to copy from principal network to target network. We need to use tensorflow scope to distinguish the computational graphs of target and principal networks. The following function builds a tensorflow operation that does the copying $ \\theta_{\\text{target}}  \\leftarrow \\theta$. You can do a soft version $\\theta \\leftarrow (1-\\tau) \\theta + \\tau \\theta_{\\text{target}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_target_update(from_scope, to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_scope)\n",
    "    op = []\n",
    "    for v1, v2 in zip(from_vars, to_vars):\n",
    "        op.append(v2.assign(v1))\n",
    "    return op    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the ingredients for DQN, we can write the main procedure to train DQN on a given environment. The implementation is straightforward if you follow the pseudocode. Refer to the pseudocode pdf for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameter initializations\n",
    "lr = 1e-3  # learning rate for gradient update\n",
    "batchsize = 64  # batchsize for buffer sampling\n",
    "maxlength = 10000  # max number of tuples held by buffer\n",
    "envname = \"CartPole-v0\"  # environment name\n",
    "tau = 100  # time steps for target update\n",
    "episodes = 3000  # number of episodes to run\n",
    "initialsize = 500  # initial time steps before start updating\n",
    "epsilon = .05  # constant for exploration\n",
    "gamma = .99  # discount\n",
    "\n",
    "# initialize environment\n",
    "env = gym.make(envname)\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n\n",
    "\n",
    "# initialize tensorflow session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# initialize networks\n",
    "with tf.variable_scope(\"principal\"):\n",
    "    Qprincipal = Qfunction(obssize, actsize, sess, optimizer)\n",
    "with tf.variable_scope(\"target\"):\n",
    "    Qtarget = Qfunction(obssize, actsize, sess, optimizer)\n",
    "\n",
    "# build ops\n",
    "update = build_target_update(\"principal\", \"target\")  # call sess.run(update) to copy\n",
    "                                                     # from principal to target\n",
    "\n",
    "# initialization of graph and buffer\n",
    "sess.run(tf.global_variables_initializer())\n",
    "buffer = ReplayBuffer(maxlength)\n",
    "sess.run(update)\n",
    "\n",
    "# main iteration\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE CODE HERE\n",
    "# after training, we will evaluate the performance of the agent\n",
    "# on a target environment\n",
    "eval_episodes = 100\n",
    "record = []\n",
    "env = gym.make('CartPole-v0')\n",
    "eval_mode = True\n",
    "for ite in range(eval_episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    \n",
    "    while not done:\n",
    "        if eval_mode:\n",
    "            values = Qprincipal.compute_Qvalues(np.expand_dims(obs,0))\n",
    "            action = np.argmax(values.flatten())\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        newobs, r, done, _ = env.step(action)\n",
    "        rsum += r\n",
    "        obs = newobs\n",
    "    \n",
    "    record.append(rsum)\n",
    "\n",
    "print(\"eval performance of DQN agent: {}\".format(np.mean(record)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
