{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lab Instruction\n",
    "\n",
    "In this notebook, we will learn how to implement Policy Gradient (PG) using Tensorflow.  We will look at a specific version -- Actor Critic Policy Gradient. The notebook contains all the info that you need to understand the basic mechanism of PG, you could also refer to the PG pdf for detailed pseudocode. **Your task** is to fill in the\n",
    "\n",
    "YOUR CODE HERE\n",
    "\n",
    "sections in the code blocks below, to complete the building of computation graph, and the implementation of other parts of PG. You are free to tweak all codes except the last block. Your are also free to tweak the hyper-parameters to improve the performance of the agent. The final block of the code evaluates the performance of the agent on an independent 100 episodes on the environment and print out the average testing performance.\n",
    "\n",
    "Make sure that your final submission is a notebook that can be run from beginning to end, and you should print out the accuracy at the end of the notebook (i.e. be sure to run the last block after training). The upper bound of your agent's performance is 500 (you could get at most 500 for this environment). **Your grade will depend on the final evaluation performance of the agent**. However, if you tweak the code to report false result, you will receive no credit for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we aim to solve CartPole-v1, a slightly more difficult versio than CartPole-v0. More specifically, CartPole-v0 is solved if the pole can be balanced for 200 steps while CartPole-v1 is for 500 steps. The state/action space for both environments are the same. Look at their dimensions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(env.observation_space)  # four observations: horizontal coordinate of car, horizontal velocity of car\n",
    "                              # angle of the pole to the vertical line, angular velocity of the pole\n",
    "print(env.action_space)  # two actions: push to the right/left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "In the previous lab, we talked about value based method for reinforcement learning. In this lab, we focus on policy based method.\n",
    "\n",
    "In policy based methods, intead of defining a value function $Q_\\theta(s,a)$ and inducing a policy based on argmax, we parameterize a stochastic policy directly. The policy is parameterized as a categorical distribution over actions. Let it be $\\pi_\\phi(s)$ with parameter $\\phi$, then the policy is defined by sampling actions $$a \\sim \\pi_\\phi(s)$$\n",
    "\n",
    "The policy induces a probability $p(\\tau)$ over trajectories $\\tau = \\{s_0,a_0,s_1,a_1...s_T,a_T\\}$ (assume horizon $T$). The expected distribution is \n",
    "\n",
    "$$R = \\mathbb{E}_{\\tau \\sim p(\\tau)} \\big[R(\\tau)\\big] = \\mathbb{E}_\\pi \\big[\\sum_{t=0}^\\infty r_t \\gamma^t \\big]$$\n",
    "\n",
    "The aim is to find $\\phi$ such that the expected reward induced by $\\pi_\\phi$ is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Computation\n",
    "\n",
    "We can derive policy gradient\n",
    "\n",
    "$$\\nabla_\\phi R = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty Q^\\pi(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(a_t|s_t) \\big]$$\n",
    "\n",
    "To compute the gradient for update $\\phi \\leftarrow \\phi + \\alpha \\nabla_\\phi R$, we need to estimate $Q^\\pi(s,a)$. Since $Q^\\pi(s,a)$ is usually not analytically accessible, it can be approximated by \n",
    "1. Monte Carlo estimate\n",
    "2. Train a value function $Q_\\theta(s,a) \\approx Q^\\pi(s,a)$ and use it as a proxy\n",
    "3. Mixture of both above\n",
    "\n",
    "Before estimating $Q^\\pi(s,a)$, let us write a parameterized policy over actions. The policy $\\pi_\\phi(s)$ takes a state as input and outputs a categorical distribution over actions. For example, if we have two actions, the probability vector to output is of the form $[0.6,0.4]$. \n",
    "\n",
    "**Loss function**\n",
    "The loss function enables us to compute policy gradients in implementation. PG has the form \n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N \\nabla_\\phi \\log \\pi_\\phi(a_i|s_i) Q_i$$\n",
    "\n",
    "where $Q_i$s are estimated and $\\nabla_\\phi \\log\\pi_\\phi(a_i|s_i)$s are computed via backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define neural net \\pi_\\phi(s) as a class\n",
    "class Policy(object):\n",
    "    def __init__(self, obssize, actsize, sess, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: size of the states\n",
    "        actsize: size of the actions\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # BUILD PREDICTION GRAPH\n",
    "        # build the input\n",
    "        state = tf.placeholder(tf.float32, [None, obssize])\n",
    "        \n",
    "        prob = None  # prob is of shape [None, actsize]\n",
    "        \n",
    "        # BUILD LOSS \n",
    "        Q_estimate = tf.placeholder(tf.float32, [None])\n",
    "        actions = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        surrogate_loss = None\n",
    "        \n",
    "        self.train_op = optimizer.minimize(surrogate_loss)\n",
    "        \n",
    "        # some bookkeeping\n",
    "        self.state = state\n",
    "        self.prob = prob\n",
    "        self.actions = actions\n",
    "        self.Q_estimate = Q_estimate\n",
    "        self.loss = surrogate_loss\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "    \n",
    "    def compute_prob(self, states):\n",
    "        \"\"\"\n",
    "        compute prob over actions given states pi(a|s)\n",
    "        states: numpy array of size [numsamples, obssize]\n",
    "        return: numpy array of size [numsamples, actsize]\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return self.sess.run(self.prob, feed_dict={self.state:states})\n",
    "\n",
    "    def train(self, states, actions, Qs):\n",
    "        \"\"\"\n",
    "        states: numpy array (states)\n",
    "        actions: numpy array (actions)\n",
    "        Qs: numpy array (Q values)\n",
    "        \"\"\"\n",
    "        self.sess.run(self.train_op, feed_dict={self.state:states, self.actions:actions, self.Q_estimate:Qs})   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to rollout trajecories using the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just pseudocode\n",
    "\"\"\"\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    prob = policy.compute_prob(obs)\n",
    "    action = np.random.randint(0, prob.size, p=prob.flatten())\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate $Q^\\pi(s,a)$\n",
    "\n",
    "To estimate $Q^\\pi(s,a)$, we can rollout the policy until the episode ends and do monte carlo estimate. In particular, under policy $\\pi$, we start from state action $(s_0,a_0)$ and rollout the policy to generate a trajectory $\\{s_0,a_0,s_1,a_1...s_T,a_T\\}$, with corresponding reward $r_0,r_1...r_T$. Monte carlo estimate is \n",
    "\n",
    "$$\\hat{Q}_{MC}(s,a) = \\sum_{t=0}^T r_t \\gamma^t \\approx Q^\\pi(s,a)$$\n",
    "\n",
    "This estimate by itself is of high variance. Using pure monte carlo estimate may work but the gradient can have large variance and hence take the algorithm  a long time to converge. We can reduce variance using baseline. Recall the derivation of PG\n",
    "\n",
    "$$\\nabla_\\phi R = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty Q^\\pi(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(a_t|s_t) \\big] = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty ( Q^\\pi(s_t,a_t) - f(s_t)) \\nabla_\\phi \\log \\pi_\\phi(a_t|s_t) \\big]$$\n",
    "\n",
    "where $f(s_t)$ can be any function of state $s_t$. $f(s_t)$ is called baseline. Optimal baseline function is hard to compute, but a good proxy is the value function $V^\\pi(s_t)$. Hence the gradient has the form \n",
    "$$\\nabla_\\phi R = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty A^\\pi(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(a_t|s_t) \\big]$$\n",
    "\n",
    "where $A^\\pi(s,a)$ is the advantage. Hence we can train a value function $V^\\pi(s)$ along side the policy and use it as baseline to reduce the variance of PG. This paradigm is **actor-critic** method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Value Function as Baseline\n",
    "\n",
    "Hence we also parameterize a value function $V_\\theta(s) \\approx V^\\pi(s)$ with parameter $\\theta$ to serve as baseline. The function takes as input the states $s$ and outputs a real value. \n",
    "\n",
    "Notice that unlike DQN, where $Q_\\theta(s,a) \\approx Q^\\ast(s,a)$ the approximated target is fixed, now we have $V_\\theta(s) \\approx V^\\pi(s)$ a moving object defined by policyb $\\pi$. If $\\pi$ is updated by PG, $\\pi$ keeps changing, which $V^\\pi(s)$ changes as well. We need to adapt $V_\\theta(s)$ online to cater for the change in policy. \n",
    "\n",
    "Recall that to evaluate a policy $\\pi$, we collect rollouts using $\\pi$. If we start with state $s_0$, the reward following $\\pi$ thereafter is $r_0,r_1...r_{T-1}$  then \n",
    "\n",
    "$$V^\\pi(s_0) \\approx \\sum_{t=0}^T r_t \\gamma^t = \\hat{V}(s_0)$$\n",
    "\n",
    "We compute $V_\\phi(s) \\approx V^\\pi(s)$ by minimizing \n",
    "\n",
    "$$(V_\\phi(s_0) - \\hat{V}(s_0))^2$$\n",
    "\n",
    "Hence, given a policy $\\pi$. Starting from $s_0$, generate trajectory $\\{s_0,a_0,s_1,a_1...a_{T-1},s_T\\}$ and rewards $\\{r_0,r_1...r_{T-1}\\}$. We can estimate the value function for state $s_i,0\\leq i\\leq T$. In general\n",
    "\n",
    "$$\\hat{V}(s_i) = \\sum_{i=t}^T r_i \\gamma^{i-t}$$\n",
    "\n",
    "And the objective to minimize over is \n",
    "$$\\frac{1}{T+1} \\sum_{i=0}^{T} (V_\\theta(s_i) - \\hat{V}(s_i))^2$$\n",
    "\n",
    "Since the policy keeps updating, we do not have to minimize the above objective to optimality. In practice, take one gradient step w.r.t. above objective suffices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define value function as a class\n",
    "class ValueFunction(object):\n",
    "    def __init__(self, obssize, sess, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: size of states\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # need to implement both prediction and loss\n",
    "        pass\n",
    "\n",
    "    def compute_values(self, states):\n",
    "        \"\"\"\n",
    "        compute value function for given states\n",
    "        states: numpy array of size [numsamples, obssize]\n",
    "        return: numpy array of size [numsamples]\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return None\n",
    "\n",
    "    def train(self, states, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array\n",
    "        targets: numpy array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of pseudocode (training procedure)\n",
    "\n",
    "The critical components of the pseudocode is as follows.\n",
    "\n",
    "**Collect trajectories** Given current policy $\\pi_\\phi$, we can rollout using the policy by executing $a_t \\sim \\pi_\\phi(s_t)$. Assume that the horizon is $T$, we can collect $N$ trajectories each with length $T+1$.\n",
    "\n",
    "**Update value function** Value function update is based on minimizing the L2 loss between predicted value function and estimated value functions. For each state in the collected sample $s_i$, estimate a value function using the rest of the path (as above). Let the estimate be $\\hat{V}(s_i)$. Then take one gradient step to update $\\theta$ using the loss\n",
    "\n",
    "$$\\frac{1}{N(T+1)} \\sum_{i=1} (V_\\theta(s_i) - \\hat{V}(s_i))^2$$\n",
    "\n",
    "**Update policy using PG** To compute PG, we need to first monte carlo estimate action-value function $\\hat{Q}(s_i,a_i)$. Then use value function as a baseline to compute advantage\n",
    "\n",
    "$$\\hat{A}(s_i,a_i) = \\hat{Q}(s_i,a_i) - V_\\theta(s_i)$$\n",
    "\n",
    "Then compute surrogate loss \n",
    "\n",
    "$$L = - \\frac{1}{N(T+1)}\\sum_{i} \\hat{A}(s_i,a_i) \\log \\pi(a_i|s_i) $$\n",
    "\n",
    "The policy is updated by $$\\phi \\leftarrow \\phi - \\alpha  \\nabla_\\phi L$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main iteration implementations\n",
    "\n",
    "Below are skeleton codes that you may find useful in implementing the above procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discounted_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for i in reversed(range(0,len(r))):\n",
    "        discounted_r[i] = running_sum * gamma + r[i]\n",
    "        running_sum = discounted_r[i]\n",
    "    return list(discounted_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE -- feel free to tune/add hyper-parameters\n",
    "# parameter initializations\n",
    "alpha = 1e-3  # learning rate for PG\n",
    "beta = 1e-3  # learning rate for baseline\n",
    "numtrajs = 10  # num of trajecories to collect at each iteration \n",
    "iterations = 1000  # total num of iterations\n",
    "envname = \"CartPole-v1\"  # environment name\n",
    "gamma = .99  # discount\n",
    "\n",
    "# initialize environment\n",
    "env = gym.make(envname)\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n\n",
    "\n",
    "# sess\n",
    "sess = tf.Session()\n",
    "\n",
    "# optimizer\n",
    "optimizer_p = tf.train.AdamOptimizer(alpha)\n",
    "optimizer_v = tf.train.AdamOptimizer(beta)\n",
    "\n",
    "# initialize networks\n",
    "actor = Policy(obssize, actsize, sess, optimizer_p)  # policy initialization\n",
    "baseline = ValueFunction(obssize, sess, optimizer_v)  # baseline initialization\n",
    "\n",
    "# initialize tensorflow graphs\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# main iteration\n",
    "for ite in range(episodes):    \n",
    "\n",
    "    # trajs records for batch update\n",
    "    OBS = []  # observations\n",
    "    ACTS = []  # actions\n",
    "    ADS = []  # advantages (to update policy)\n",
    "    VAL = []  # value functions (to update baseline)\n",
    "\n",
    "    for num in range(numtrajs):\n",
    "        # record for each episode\n",
    "        obss = []  # observations\n",
    "        acts = []   # actions\n",
    "        rews = []  # instant rewards\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            prob = actor.compute_prob(np.expand_dims(obs,0))\n",
    "            action = np.random.choice(actsize, p=prob.flatten(), size=1)\n",
    "            newobs, reward, done, _ = env.step(action[0])\n",
    "\n",
    "            # record\n",
    "            obss.append(obs)\n",
    "            acts.append(action[0])\n",
    "            rews.append(reward)\n",
    "\n",
    "            # update\n",
    "            obs = newobs\n",
    "\n",
    "        # compute returns from instant rewards\n",
    "        returns = discounted_rewards(rews, gamma)\n",
    "    \n",
    "        # record for batch update\n",
    "        VAL += returns\n",
    "        OBS += obss\n",
    "        ACTS += acts\n",
    "    \n",
    "    # update baseline\n",
    "    VAL = np.array(VAL)\n",
    "    OBS = np.array(OBS)\n",
    "    ACTS = np.array(ACTS)\n",
    "    \n",
    "    baseline.train(OBS, VAL)  # update only one step\n",
    "    \n",
    "    # update policy\n",
    "    BAS = baseline.compute_values(OBS)  # compute baseline for variance reduction\n",
    "    ADS = VAL - np.squeeze(BAS,1)\n",
    "    \n",
    "    actor.train(OBS, ACTS, ADS)  # update only one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE CODE HERE\n",
    "# after training, we will evaluate the performance of the agent\n",
    "# on a target environment\n",
    "eval_episodes = 100\n",
    "record = []\n",
    "env = gym.make('CartPole-v1')\n",
    "eval_mode = True\n",
    "for ite in range(eval_episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # epsilon greedy for exploration\n",
    "        if eval_mode:\n",
    "            p = actor.compute_prob(np.expand_dims(obs,0)).ravel()\n",
    "            action = np.random.choice(np.arange(2), size=1, p=p)[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        newobs, r, done, _ = env.step(action)\n",
    "        rsum += r\n",
    "        obs = newobs\n",
    "    \n",
    "    record.append(rsum)\n",
    "\n",
    "print(\"eval performance of PG agent: {}\".format(np.mean(record)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
