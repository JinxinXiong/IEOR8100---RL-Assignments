{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "lab4-policy gradient.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2PKgK2vENdY"
      },
      "source": [
        "#  Lab Instruction\n",
        "\n",
        "In this notebook, we will learn how to implement Policy Gradient (PG) using Tensorflow.  We will look at a specific version -- Actor Critic Policy Gradient. The notebook contains all the info that you need to understand the basic mechanism of PG, you could also refer to the PG pdf for detailed pseudocode. **Your task** is to fill in the\n",
        "\n",
        "YOUR CODE HERE\n",
        "\n",
        "sections in the code blocks below, to complete the building of computation graph, and the implementation of other parts of PG. You are free to tweak all codes except the last block. Your are also free to tweak the hyper-parameters to improve the performance of the agent. The final block of the code evaluates the performance of the agent on an independent 100 episodes on the environment and print out the average testing performance.\n",
        "\n",
        "Make sure that your final submission is a notebook that can be run from beginning to end, and you should print out the accuracy at the end of the notebook (i.e. be sure to run the last block after training). The upper bound of your agent's performance is 500 (you could get at most 500 for this environment). **Your grade will depend on the final evaluation performance of the agent**. However, if you tweak the code to report false result, you will receive no credit for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVdf3kuTEvdk",
        "outputId": "15a8c610-1ced-473b-94f5-6396108a2e59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow==1.15.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.15.0 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.15.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.33.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.18.5)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.35.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zXCB5MuENdc",
        "outputId": "1a048143-1de0-4b22-eae2-db1a300114f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLOF8etFENdh"
      },
      "source": [
        "In this assignment, we aim to solve CartPole-v1, a slightly more difficult versio than CartPole-v0. More specifically, CartPole-v0 is solved if the pole can be balanced for 200 steps while CartPole-v1 is for 500 steps. The state/action space for both environments are the same. Look at their dimensions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPGtHn6LENdh",
        "outputId": "c1528b00-4f03-480f-b9fe-f3aaf96cba73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "print(env.observation_space)  # four observations: horizontal coordinate of car, horizontal velocity of car\n",
        "                              # angle of the pole to the vertical line, angular velocity of the pole\n",
        "print(env.action_space)  # two actions: push to the right/left"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Discrete(2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAxcz-c9ENdk"
      },
      "source": [
        "## Policy Gradient\n",
        "\n",
        "In the previous lab, we talked about value based method for reinforcement learning. In this lab, we focus on policy based method.\n",
        "\n",
        "In policy based methods, intead of defining a value function $Q_\\theta(s,a)$ and inducing a policy based on argmax, we parameterize a stochastic policy directly. The policy is parameterized as a categorical distribution over actions. Let it be $\\pi_\\phi(s)$ with parameter $\\phi$, then the policy is defined by sampling actions $$a \\sim \\pi_\\phi(s)$$\n",
        "\n",
        "The policy induces a probability $p(\\tau)$ over trajectories $\\tau = \\{s_0,a_0,s_1,a_1...s_T,a_T\\}$ (assume horizon $T$). The expected distribution is \n",
        "\n",
        "$$R = \\mathbb{E}_{\\tau \\sim p(\\tau)} \\big[R(\\tau)\\big] = \\mathbb{E}_\\pi \\big[\\sum_{t=0}^\\infty r_t \\gamma^t \\big]$$\n",
        "\n",
        "The aim is to find $\\phi$ such that the expected reward induced by $\\pi_\\phi$ is maximized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffOhX52BENdl"
      },
      "source": [
        "### Policy Gradient Computation\n",
        "\n",
        "We can derive policy gradient\n",
        "\n",
        "$$\\nabla_\\phi R = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty Q^\\pi(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(a_t|s_t) \\big]$$\n",
        "\n",
        "To compute the gradient for update $\\phi \\leftarrow \\phi + \\alpha \\nabla_\\phi R$, we need to estimate $Q^\\pi(s,a)$. Since $Q^\\pi(s,a)$ is usually not analytically accessible, it can be approximated by \n",
        "1. Monte Carlo estimate\n",
        "2. Train a value function $Q_\\theta(s,a) \\approx Q^\\pi(s,a)$ and use it as a proxy\n",
        "3. Mixture of both above\n",
        "\n",
        "Before estimating $Q^\\pi(s,a)$, let us write a parameterized policy over actions. The policy $\\pi_\\phi(s)$ takes a state as input and outputs a categorical distribution over actions. For example, if we have two actions, the probability vector to output is of the form $[0.6,0.4]$. \n",
        "\n",
        "**Loss function**\n",
        "The loss function enables us to compute policy gradients in implementation. PG has the form \n",
        "\n",
        "$$\\frac{1}{N} \\sum_{i=1}^N \\nabla_\\phi \\log \\pi_\\phi(a_i|s_i) Q_i$$\n",
        "\n",
        "where $Q_i$s are estimated and $\\nabla_\\phi \\log\\pi_\\phi(a_i|s_i)$s are computed via backprop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InJSwzIjENdl"
      },
      "source": [
        "# define neural net \\pi_\\phi(s) as a class\n",
        "class Policy(object):\n",
        "    def __init__(self, obssize, actsize, sess, optimizer):\n",
        "        \"\"\"\n",
        "        obssize: size of the states\n",
        "        actsize: size of the actions\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # BUILD PREDICTION GRAPH\n",
        "        # build the input\n",
        "        state = tf.placeholder(tf.float32, [None, obssize])\n",
        "        \n",
        "        hidden = 128\n",
        "        # prob = None  # prob is of shape [None, actsize]\n",
        "        self.W1 = tf.Variable(tf.truncated_normal([obssize, hidden]), name='W1')*0.1\n",
        "        self.b1 = tf.Variable(tf.zeros([hidden]), name='b1')\n",
        "        self.h1 = tf.matmul(state, self.W1)+self.b1\n",
        "        self.W2 = tf.Variable(tf.truncated_normal([hidden, actsize]), name='W2')*0.1\n",
        "        self.b2 = tf.Variable(tf.zeros([actsize]), name='b2')\n",
        "        self.h2 = tf.matmul(self.h1, self.W2) + self.b2\n",
        "        prob = tf.nn.softmax(self.h2)\n",
        "\n",
        "        # BUILD LOSS \n",
        "        Q_estimate = tf.placeholder(tf.float32, [None])\n",
        "        actions = tf.placeholder(tf.int32, [None])\n",
        "\n",
        "        # surrogate_loss = None\n",
        "        surrogate_loss = -tf.reduce_mean(tf.math.log(tf.reduce_sum(prob*tf.one_hot(actions, actsize), axis=1))*Q_estimate)\n",
        "        \n",
        "        self.train_op = optimizer.minimize(surrogate_loss)\n",
        "        \n",
        "        # some bookkeeping\n",
        "        self.state = state\n",
        "        self.prob = prob\n",
        "        self.actions = actions\n",
        "        self.Q_estimate = Q_estimate\n",
        "        self.loss = surrogate_loss\n",
        "        self.optimizer = optimizer\n",
        "        self.sess = sess\n",
        "    \n",
        "    def compute_prob(self, states):\n",
        "        \"\"\"\n",
        "        compute prob over actions given states pi(a|s)\n",
        "        states: numpy array of size [numsamples, obssize]\n",
        "        return: numpy array of size [numsamples, actsize]\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        return self.sess.run(self.prob, feed_dict={self.state: states})\n",
        "\n",
        "    def train(self, states, actions, Qs):\n",
        "        \"\"\"\n",
        "        states: numpy array (states)\n",
        "        actions: numpy array (actions)\n",
        "        Qs: numpy array (Q values)\n",
        "        \"\"\"\n",
        "        return self.sess.run(self.train_op, feed_dict={self.state: states, self.actions: actions, self.Q_estimate: Qs})   "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibWPbsIbENdo"
      },
      "source": [
        "Try to rollout trajecories using the policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVkNSrK5ENdp",
        "outputId": "3c330c6a-1f31-4581-d49b-850e0334de84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# just pseudocode\n",
        "\"\"\"\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    prob = policy.compute_prob(obs)\n",
        "    action = np.random.randint(0, prob.size, p=prob.flatten())\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "obssize = 4\n",
        "actsize = 2\n",
        "sess = tf.Session()\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "policy = Policy(obssize, actsize, sess, optimizer)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "obs = env.reset()\n",
        "done = False    \n",
        "sum_reward = 0\n",
        "while not done:\n",
        "    obs = np.expand_dims(obs, 0)\n",
        "    prob = policy.compute_prob(obs)\n",
        "    action = np.random.choice(prob.size, p=prob.flatten())\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    sum_reward += reward\n",
        "print(sum_reward)\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nobssize = 4\\nactsize = 2\\nsess = tf.Session()\\n\\noptimizer = tf.train.AdamOptimizer()\\n\\nenv = gym.make(\"CartPole-v1\")\\n\\npolicy = Policy(obssize, actsize, sess, optimizer)\\nsess.run(tf.global_variables_initializer())\\n\\nobs = env.reset()\\ndone = False    \\nsum_reward = 0\\nwhile not done:\\n    obs = np.expand_dims(obs, 0)\\n    prob = policy.compute_prob(obs)\\n    action = np.random.choice(prob.size, p=prob.flatten())\\n    obs, reward, done, info = env.step(action)\\n    sum_reward += reward\\nprint(sum_reward)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2lMqFODENdr"
      },
      "source": [
        "### Estimate $Q^\\pi(s,a)$\n",
        "\n",
        "To estimate $Q^\\pi(s,a)$, we can rollout the policy until the episode ends and do monte carlo estimate. In particular, under policy $\\pi$, we start from state action $(s_0,a_0)$ and rollout the policy to generate a trajectory $\\{s_0,a_0,s_1,a_1...s_T,a_T\\}$, with corresponding reward $r_0,r_1...r_T$. Monte carlo estimate is \n",
        "\n",
        "$$\\hat{Q}_{MC}(s,a) = \\sum_{t=0}^T r_t \\gamma^t \\approx Q^\\pi(s,a)$$\n",
        "\n",
        "This estimate by itself is of high variance. Using pure monte carlo estimate may work but the gradient can have large variance and hence take the algorithm  a long time to converge. We can reduce variance using baseline. Recall the derivation of PG\n",
        "\n",
        "$$\\nabla_\\phi R = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty Q^\\pi(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(a_t|s_t) \\big] = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty ( Q^\\pi(s_t,a_t) - f(s_t)) \\nabla_\\phi \\log \\pi_\\phi(a_t|s_t) \\big]$$\n",
        "\n",
        "where $f(s_t)$ can be any function of state $s_t$. $f(s_t)$ is called baseline. Optimal baseline function is hard to compute, but a good proxy is the value function $V^\\pi(s_t)$. Hence the gradient has the form \n",
        "$$\\nabla_\\phi R = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty A^\\pi(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(a_t|s_t) \\big]$$\n",
        "\n",
        "where $A^\\pi(s,a)$ is the advantage. Hence we can train a value function $V^\\pi(s)$ along side the policy and use it as baseline to reduce the variance of PG. This paradigm is **actor-critic** method. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFwXqz_5ENds"
      },
      "source": [
        "### Train Value Function as Baseline\n",
        "\n",
        "Hence we also parameterize a value function $V_\\theta(s) \\approx V^\\pi(s)$ with parameter $\\theta$ to serve as baseline. The function takes as input the states $s$ and outputs a real value. \n",
        "\n",
        "Notice that unlike DQN, where $Q_\\theta(s,a) \\approx Q^\\ast(s,a)$ the approximated target is fixed, now we have $V_\\theta(s) \\approx V^\\pi(s)$ a moving object defined by policyb $\\pi$. If $\\pi$ is updated by PG, $\\pi$ keeps changing, which $V^\\pi(s)$ changes as well. We need to adapt $V_\\theta(s)$ online to cater for the change in policy. \n",
        "\n",
        "Recall that to evaluate a policy $\\pi$, we collect rollouts using $\\pi$. If we start with state $s_0$, the reward following $\\pi$ thereafter is $r_0,r_1...r_{T-1}$  then \n",
        "\n",
        "$$V^\\pi(s_0) \\approx \\sum_{t=0}^T r_t \\gamma^t = \\hat{V}(s_0)$$\n",
        "\n",
        "We compute $V_\\phi(s) \\approx V^\\pi(s)$ by minimizing \n",
        "\n",
        "$$(V_\\phi(s_0) - \\hat{V}(s_0))^2$$\n",
        "\n",
        "Hence, given a policy $\\pi$. Starting from $s_0$, generate trajectory $\\{s_0,a_0,s_1,a_1...a_{T-1},s_T\\}$ and rewards $\\{r_0,r_1...r_{T-1}\\}$. We can estimate the value function for state $s_i,0\\leq i\\leq T$. In general\n",
        "\n",
        "$$\\hat{V}(s_i) = \\sum_{i=t}^T r_i \\gamma^{i-t}$$\n",
        "\n",
        "And the objective to minimize over is \n",
        "$$\\frac{1}{T+1} \\sum_{i=0}^{T} (V_\\theta(s_i) - \\hat{V}(s_i))^2$$\n",
        "\n",
        "Since the policy keeps updating, we do not have to minimize the above objective to optimality. In practice, take one gradient step w.r.t. above objective suffices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9byyXWOKENds"
      },
      "source": [
        "# define value function as a class\n",
        "class ValueFunction(object):\n",
        "    def __init__(self, obssize, sess, optimizer):\n",
        "        \"\"\"\n",
        "        obssize: size of states\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # need to implement both prediction and loss\n",
        "        # pass\n",
        "        hidden=128\n",
        "        state = tf.placeholder(tf.float32, [None, obssize])\n",
        "        self.W1 = tf.Variable(tf.truncated_normal([obssize, hidden]), name='Value_W1')*0.1\n",
        "        self.b1 = tf.Variable(tf.zeros([hidden]), name='Value_b1')\n",
        "        self.h1 = tf.nn.relu(tf.matmul(state, self.W1)+self.b1)\n",
        "        self.W2 = tf.Variable(tf.truncated_normal([hidden, 1]), name='Value_W2')*0.1\n",
        "        self.b2 = tf.Variable(tf.zeros([1]), name='Value_b2')\n",
        "        value = tf.matmul(self.h1, self.W2) + self.b2\n",
        "\n",
        "        target = tf.placeholder(tf.float32, [None])\n",
        "        loss = tf.reduce_mean(tf.square(value-target))\n",
        "\n",
        "        self.train_op = optimizer.minimize(loss)\n",
        "\n",
        "        self.obssize = obssize\n",
        "        self.sess = sess\n",
        "        self.optimizer = optimizer\n",
        "        self.state = state\n",
        "        self.value = value\n",
        "        self.target = target\n",
        "        self.loss = loss\n",
        "\n",
        "    def compute_values(self, states):\n",
        "        \"\"\"\n",
        "        compute value function for given states\n",
        "        states: numpy array of size [numsamples, obssize]\n",
        "        return: numpy array of size [numsamples]\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # return None\n",
        "        return self.sess.run(self.value, feed_dict={self.state: states})\n",
        "\n",
        "    def train(self, states, targets):\n",
        "        \"\"\"\n",
        "        states: numpy array\n",
        "        targets: numpy array\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # pass\n",
        "        return self.sess.run([self.loss, self.train_op], feed_dict={self.state: states, self.target: targets})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5GHG9lHUWd3"
      },
      "source": [
        "# obssize = 4\n",
        "# actsize = 2\n",
        "# sess = tf.Session()\n",
        "\n",
        "# optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# value = ValueFunction(obssize, sess, optimizer)\n",
        "# sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# obs = env.reset()\n",
        "# done = False    \n",
        "# obs = np.expand_dims(obs, 0)\n",
        "\n",
        "# value.compute_values(obs)\n",
        "# value.train(obs, np.array([[1.]]))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khBZuMmtENdv"
      },
      "source": [
        "### Summary of pseudocode (training procedure)\n",
        "\n",
        "The critical components of the pseudocode is as follows.\n",
        "\n",
        "**Collect trajectories** Given current policy $\\pi_\\phi$, we can rollout using the policy by executing $a_t \\sim \\pi_\\phi(s_t)$. Assume that the horizon is $T$, we can collect $N$ trajectories each with length $T+1$.\n",
        "\n",
        "**Update value function** Value function update is based on minimizing the L2 loss between predicted value function and estimated value functions. For each state in the collected sample $s_i$, estimate a value function using the rest of the path (as above). Let the estimate be $\\hat{V}(s_i)$. Then take one gradient step to update $\\theta$ using the loss\n",
        "\n",
        "$$\\frac{1}{N(T+1)} \\sum_{i=1} (V_\\theta(s_i) - \\hat{V}(s_i))^2$$\n",
        "\n",
        "**Update policy using PG** To compute PG, we need to first monte carlo estimate action-value function $\\hat{Q}(s_i,a_i)$. Then use value function as a baseline to compute advantage\n",
        "\n",
        "$$\\hat{A}(s_i,a_i) = \\hat{Q}(s_i,a_i) - V_\\theta(s_i)$$\n",
        "\n",
        "Then compute surrogate loss \n",
        "\n",
        "$$L = - \\frac{1}{N(T+1)}\\sum_{i} \\hat{A}(s_i,a_i) \\log \\pi(a_i|s_i) $$\n",
        "\n",
        "The policy is updated by $$\\phi \\leftarrow \\phi - \\alpha  \\nabla_\\phi L$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8TZPFSkENdv"
      },
      "source": [
        "## Main iteration implementations\n",
        "\n",
        "Below are skeleton codes that you may find useful in implementing the above procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfj7wGVRENdx"
      },
      "source": [
        "# function to calculate the value function\n",
        "def discounted_rewards(r, gamma):\n",
        "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r)\n",
        "    running_sum = 0\n",
        "    for i in reversed(range(0,len(r))):\n",
        "        discounted_r[i] = running_sum * gamma + r[i]\n",
        "        running_sum = discounted_r[i]\n",
        "    return list(discounted_r)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFeVUYwmENd0",
        "outputId": "e652dbaf-aee0-442b-d884-4f7d52e67a88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# YOUR CODE HERE -- feel free to tune/add hyper-parameters\n",
        "# parameter initializations\n",
        "alpha = 1e-2  # learning rate for PG\n",
        "beta = 1e-2  # learning rate for baseline\n",
        "numtrajs = 1 # num of trajecories to collect at each iteration \n",
        "iterations = 5000  # total num of iterations\n",
        "envname = \"CartPole-v1\"  # environment name\n",
        "gamma = .99  # discount\n",
        "# episodes = 1000\n",
        "\n",
        "\n",
        "# initialize environment\n",
        "env = gym.make(envname)\n",
        "obssize = env.observation_space.low.size\n",
        "actsize = env.action_space.n\n",
        "\n",
        "# sess\n",
        "sess = tf.Session()\n",
        "\n",
        "# optimizer\n",
        "optimizer_p = tf.train.AdamOptimizer(alpha)\n",
        "optimizer_v = tf.train.AdamOptimizer(beta)\n",
        "\n",
        "# initialize networks\n",
        "actor = Policy(obssize, actsize, sess, optimizer_p)  # policy initialization\n",
        "baseline = ValueFunction(obssize, sess, optimizer_v)  # baseline initialization\n",
        "\n",
        "# initialize tensorflow graphs\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "solved = False\n",
        "running_reward = 0\n",
        "history = []\n",
        "# main iteration\n",
        "# for ite in range(episodes):   \n",
        "ite = 1 \n",
        "while not solved and ite < iterations:\n",
        "\n",
        "    # trajs records for batch update\n",
        "    OBS = []  # observations\n",
        "    ACTS = []  # actions\n",
        "    ADS = []  # advantages (to update policy)\n",
        "    VAL = []  # value functions (to update baseline)\n",
        "\n",
        "    episode_reward = []\n",
        "    for num in range(numtrajs):\n",
        "        # record for each episode\n",
        "        obss = []  # observations\n",
        "        acts = []   # actions\n",
        "        rews = []  # instant rewards\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "        # rollout one trajectory\n",
        "        while not done:\n",
        "            prob = actor.compute_prob(np.expand_dims(obs, 0))\n",
        "            action = np.random.choice(actsize, p=prob.flatten(), size=1)\n",
        "            newobs, reward, done, _ = env.step(action[0])\n",
        "            \n",
        "            # record\n",
        "            obss.append(obs)\n",
        "            acts.append(action[0])\n",
        "            rews.append(reward)\n",
        "\n",
        "            # update\n",
        "            obs = newobs\n",
        "        # compute returns from instant rewards\n",
        "        returns = discounted_rewards(rews, gamma)\n",
        "        episode_reward.append(np.sum(rews))\n",
        "        # record for batch update\n",
        "        VAL += returns\n",
        "        OBS += obss\n",
        "        ACTS += acts\n",
        "        \n",
        "    # print(np.mean(episode_reward))\n",
        "    ite += 1\n",
        "    # update baseline\n",
        "    VAL = np.array(VAL)\n",
        "    OBS = np.array(OBS)\n",
        "    ACTS = np.array(ACTS)\n",
        "    \n",
        "    loss, _ = baseline.train(OBS, VAL)  # update only one step\n",
        "    # update policy\n",
        "    BAS = baseline.compute_values(OBS)  # compute baseline for variance reduction\n",
        "    ADS = VAL - np.squeeze(BAS, 1)\n",
        "    \n",
        "    actor.train(OBS, ACTS, ADS)  # update only one step\n",
        "\n",
        "    running_reward = 0.05*np.mean(episode_reward) + (1-0.05)*running_reward\n",
        "    history.append(running_reward)\n",
        "    if ite%10 == 0:\n",
        "        print(\"iteration \"+str(ite)+': ', running_reward)\n",
        "    if running_reward > 499.9:\n",
        "        print(\"solved at iteration {}\".format(ite))\n",
        "        break\n",
        "    if ite > iterations:\n",
        "        print(\"not solved after {} episodes\".format(iterations))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 10:  8.979632674816406\n",
            "iteration 20:  15.541752780842007\n",
            "iteration 30:  19.499581256488987\n",
            "iteration 40:  22.694431867254632\n",
            "iteration 50:  22.198903178801523\n",
            "iteration 60:  23.86849223520355\n",
            "iteration 70:  23.033896046930717\n",
            "iteration 80:  28.728794353536003\n",
            "iteration 90:  32.134502804360785\n",
            "iteration 100:  39.1274402134912\n",
            "iteration 110:  46.61490142987507\n",
            "iteration 120:  50.73188307315717\n",
            "iteration 130:  58.31205929791655\n",
            "iteration 140:  63.856148284047656\n",
            "iteration 150:  65.13640599054068\n",
            "iteration 160:  103.824413433712\n",
            "iteration 170:  142.3328170718884\n",
            "iteration 180:  176.65032570757955\n",
            "iteration 190:  183.88476835825267\n",
            "iteration 200:  185.18760921079777\n",
            "iteration 210:  243.03962346857892\n",
            "iteration 220:  303.789658629762\n",
            "iteration 230:  320.4142784466675\n",
            "iteration 240:  370.8778947462418\n",
            "iteration 250:  364.49938515556465\n",
            "iteration 260:  377.7247328531241\n",
            "iteration 270:  401.887536053495\n",
            "iteration 280:  418.2729686967661\n",
            "iteration 290:  411.9203824244624\n",
            "iteration 300:  440.36347936353553\n",
            "iteration 310:  429.13496390635925\n",
            "iteration 320:  438.3129380027762\n",
            "iteration 330:  429.11340960290636\n",
            "iteration 340:  427.6166572388494\n",
            "iteration 350:  447.3933717913511\n",
            "iteration 360:  468.502468442702\n",
            "iteration 370:  481.141264361819\n",
            "iteration 380:  485.65857834608966\n",
            "iteration 390:  451.92263609461054\n",
            "iteration 400:  331.14970854219916\n",
            "iteration 410:  318.2687207164758\n",
            "iteration 420:  279.0302808341841\n",
            "iteration 430:  354.3674916666196\n",
            "iteration 440:  409.207755519364\n",
            "iteration 450:  437.8631353525382\n",
            "iteration 460:  462.79636384710915\n",
            "iteration 470:  457.8432490737796\n",
            "iteration 480:  466.2255142239342\n",
            "iteration 490:  469.5275927620881\n",
            "iteration 500:  470.33923708774046\n",
            "iteration 510:  482.2410055984383\n",
            "iteration 520:  476.1620340480573\n",
            "iteration 530:  485.7273292282649\n",
            "iteration 540:  477.71229478737405\n",
            "iteration 550:  471.6372511042281\n",
            "iteration 560:  483.0181745377586\n",
            "iteration 570:  489.83235380005704\n",
            "iteration 580:  488.21071088498724\n",
            "iteration 590:  492.94131711948074\n",
            "iteration 600:  495.77370581706333\n",
            "iteration 610:  497.46956155658734\n",
            "iteration 620:  493.3948397723945\n",
            "iteration 630:  496.04524658214416\n",
            "iteration 640:  497.6321430431503\n",
            "iteration 650:  489.3742834168513\n",
            "iteration 660:  491.36839476348615\n",
            "iteration 670:  494.83193909997556\n",
            "iteration 680:  496.90569103492174\n",
            "iteration 690:  488.658325108691\n",
            "iteration 700:  493.2093202897407\n",
            "iteration 710:  495.934169214931\n",
            "iteration 720:  497.5656369202864\n",
            "iteration 730:  498.5424569006572\n",
            "iteration 740:  499.12731510589134\n",
            "iteration 750:  475.2399496134078\n",
            "iteration 760:  482.7659795446201\n",
            "iteration 770:  442.66245909177405\n",
            "iteration 780:  451.6700666018759\n",
            "iteration 790:  471.0630836036122\n",
            "iteration 800:  482.6743992458298\n",
            "iteration 810:  478.23214783398186\n",
            "iteration 820:  486.9667828203246\n",
            "iteration 830:  492.1965314374119\n",
            "iteration 840:  495.327775117393\n",
            "iteration 850:  497.20256637435426\n",
            "iteration 860:  498.3250731532582\n",
            "iteration 870:  498.99715942633344\n",
            "iteration 880:  499.39956230437855\n",
            "iteration 890:  499.6404957719201\n",
            "iteration 900:  499.16725153883607\n",
            "iteration 910:  499.50140273520714\n",
            "iteration 920:  485.97914353152294\n",
            "iteration 930:  491.60519531256324\n",
            "iteration 940:  494.97372033594\n",
            "iteration 950:  496.99058069818454\n",
            "iteration 960:  477.0796494983459\n",
            "iteration 970:  486.2767394943686\n",
            "iteration 980:  491.78337700848715\n",
            "iteration 990:  495.08040429918583\n",
            "iteration 1000:  497.054456327804\n",
            "iteration 1010:  498.23639419731626\n",
            "iteration 1020:  498.9440640596779\n",
            "iteration 1030:  499.3677721470596\n",
            "iteration 1040:  499.6214618304291\n",
            "iteration 1050:  499.7733552149661\n",
            "iteration 1060:  499.8642993951143\n",
            "solved at iteration 1066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgdb9uRsg8s6",
        "outputId": "571c2cfd-f22f-4703-b680-6b7be685d275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(history)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0e9e939e80>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5xEfeVoENd2",
        "outputId": "76dc5e88-8f8b-40ae-92d0-7e1fe934e5da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# DO NOT CHANGE CODE HERE\n",
        "# after training, we will evaluate the performance of the agent\n",
        "# on a target environment\n",
        "eval_episodes = 100\n",
        "record = []\n",
        "env = gym.make('CartPole-v1')\n",
        "eval_mode = True\n",
        "for ite in range(eval_episodes):\n",
        "    \n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    rsum = 0\n",
        "    \n",
        "    while not done:\n",
        "        \n",
        "        # epsilon greedy for exploration\n",
        "        if eval_mode:\n",
        "            p = actor.compute_prob(np.expand_dims(obs,0)).ravel()\n",
        "            action = np.random.choice(np.arange(2), size=1, p=p)[0]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        newobs, r, done, _ = env.step(action)\n",
        "        rsum += r\n",
        "        obs = newobs\n",
        "    \n",
        "    record.append(rsum)\n",
        "\n",
        "print(\"eval performance of PG agent: {}\".format(np.mean(record)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eval performance of PG agent: 500.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0Anmh0JkrDv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}